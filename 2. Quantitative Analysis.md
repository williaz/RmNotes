### the time value of money
- time value of money(TVM)
- at the end of the investment horizon (FV - compounding.) or at the beginning of the investment horizon (PV - discounting).
- think of I/Y as the interest rate per compounding period and N as the number of compounding periods under analysis
- keys
  - N = Number of compounding periods.
  - I/Y = Interest rate per compounding period.
  - PV = Present value.
  - FV = Future value.
  - PMT = Annuity payments, or constant periodic cash flow.
  - CPT = Compute.

- Equilibrium interest rates are the required rate of return for a particular investment
  - interest rates = discount rates, opportunity cost of current consumption, required rate of return, and the cost of capital
- The real risk-free rate of interest is a theoretical rate on a single period loan that has no expectation of inflation in it.
  - T-bill(U.S. Treasury bills) rates are nominal risk-free rates because they contain an inflation premium.
- Default risk: The risk that a borrower will not make the promised payments in a timely manner.
- Liquidity risk: The risk of receiving less than fair value for an investment if it must be sold for cash quickly.
- Maturity risk: the prices of longer-term bonds are more volatile than those of shorter-term bonds
- An annuity is a stream of **equal** cash flows that occurs at equal intervals over a given period.
  - The ordinary annuity is the most common type of annuity. It is characterized by cash flows that occur at the end of each compounding period.
  - the PMT variable is a single periodic payment
- A perpetuity is a financial instrument that pays a fixed amount of money at set intervals over an **infinite** period of time.
  - British consul bonds and most preferred stocks are examples of perpetuities since they promise fixed interest or dividend payments forever.
  - PVperp = PMT/(I/Y)
- to find the PV or FV of this cash flow stream, all we need to do is sum the PVs or FVs of the individual cash flows
- since an increase in the frequency of compounding increases the effective rate of interest, it also increases the FV of a given cash flow and decreases the PV of a given cash flow.
  - m equal the number of compounding periods per year,
  - I/Y = the annual interest rate / m
  - N = the number of years x m
- PV and FV must in diff sign

### random variables
#### LO 15.1: Describe and distinguish between continuous and discrete random variables.
- A random variable is an uncertain quantity/number.
- An outcome is an observed value of a random variable.
- An event is a single outcome or a set of outcomes.
- Mutually exclusive events are events that cannot happen at the same time.
- Exhaustive events are those that include all possible outcomes.
- A probability distribution describes the probabilities of all the possible outcomes for a random variable.
- A discrete random variable is one for which the number of possible outcomes can be counted, and for each possible outcome, there is a measurable and positive probability.
- A probability function, denoted p(x), specifies the probability that a random variable is equal to a specific value.
- A continuous random variable is one for which the number of possible outcomes is infinite, even if lower and upper bounds exist.

- It is customary to speak in terms of the probability of a range of possible price change


### distribution function
#### LO 15.2: Define and distinguish between the probability density function, the
cumulative distribution function, and the inverse cumulative distribution function.

- A probability density function (pdf) is a function, denoted f(x), that can be used to generate the probability that outcomes of a continuous distribution lie within a particular range of outcomes.
  - For a discrete distribution, it is the equivalent of a probability function for a discrete distribution.
  - for a continuous distribution, the probability
of any one particular outcome (of the infinite possible outcomes) is zero. A pdf is used to calculate the probability of an outcome between two values
- A cumulative distribution function (cdf), or simply distribution function, defines the probability that a random variable, X, takes on a value equal to or less than a specific value, x.
- inverse cumulative distribution function can be used to find the value that corresponds to a specific probability.


### discrete probability function
#### LO 15.3: Calculate the probability of an event given a discrete probability function.
- A discrete uniform random variable is one for which the probabilities for all possible outcomes for a discrete random variable are equal.

### conditional probability
#### LO 15.6: Define and calculate a conditional probability, and distinguish between conditional and unconditional probabilities.
- Unconditional probability (i.e., marginal probability) refers to the probability of an event regardless of the past or future occurrence of other events.
- A conditional probability is one where the occurrence of one event affects the probability of the occurrence of another event.
  - P(A | B), where the vertical bar (|) indicates “given,” or “conditional upon.”
  - A conditional probability of an occurrence is also called its likelihood.
- The joint probability of two events is the probability that they will both occur.
  - multiplication rule o f probability.
  - P(AB) = P(A | B) x P(B)

### independent and mutually exclusive events
#### LO 15.4: Distinguish between independent and mutually exclusive events.
- Independent events refer to events for which the occurrence of one has no influence on the occurrence of the others.
- The addition rule fo r probabilities is used to determine the probability that at least one of two events will occur.

### calculating a joint probability of anu number of independent events
#### LO 15.5: Define joint probability, describe a probability matrix, and calculate joint probabilities using probability matrices.
- Joint probabilities of independent events can be conveniently summarized using a probability matrix

- statistics
  - data
  - Statistical methods
    - Descriptive statistics are used to summarize the important characteristics of large data sets.
    - Inferential statistics, which will be discussed in subsequent topics, pertain to the procedures used to make forecasts, estimates, or judgments about a large set of data on the basis of the statistical characteristics of a smaller set (a sample).
- A population is defined as the set of all possible members of a stated group.
- A sample is defined as a subset of the population of interest.


### measure of central tendency
#### LO 16.1: Interpret and apply the mean, standard deviation, and variance of a random variable.
#### LO 16.2: Calculate the mean, standard deviation, and variance of a discrete random variable.
- The sum of the deviations of each observation in the data set from the mean is always zero.
- The median is the midpoint of a data set when the data is arranged in ascending or descending order.
- The mode is the value that occurs most frequently in a data set.
  - unimodal, bimodal, trimodal for one,two three value
- The geometric mean is often used when calculating investment returns over multiple periods or when measuring compound growth rates.
  - When calculating the geometric mean for a returns data set, it is necessary to add 1 to each value under the radical and then subtract 1 from the result.
  - The geom etric mean is always less than or equal to the arithmetic mean, and the difference increases as the dispersion o f the observations increases.

### expectations
#### LO 16.3: Interpret and calculate the expected value of a discrete random variable.
#### LO 16.5: Calculate the mean and variance of sums of variables.

- E(X^2) =/= [E(X)]^2
- The mean and variance of a distribution are defined as the first and second moments of the distribution, respectively.
- Var(X) = E\[(X - E(X))^2] = E(X^2) - \[E(X)]^2
- Var(aX + c) = a2 x Var(X)
- Var(X - Y) = Var(X) + Var(Y), X, Y independent
- The variance and standard deviation measure the dispersion, or volatility, of only one variable.

### covariance and correlation
#### LO 16.4: Calculate and interpret the covariance and correlation between two random variables.
- Covariance is the expected value of the product of the deviations of the two random variables from their respective expected values.
- Cov(Ri, Rj) = E{ \[Ri - E(Ri)]\[Rj - E(Rj)] } = E(Ri, Rj) - E(Ri)*E(Rj)
- Cov(X,X) = Var(X)
- Cov(a + bX, c + dY) = b x d x Cov(X,Y)
- Correlation measures the strength of the linear relationship between two random variables.
  - ranges from -1 to +1.
  - has no units.


### central moments
#### LO 16.6: Describe the four central moments of a statistical variable or distribution: mean, variance, skewness and kurtosis.
- Raw moments are measured relative to an expected value raised to the appropriate power. 
  - The first raw moment is the mean of the distribution, which is the expected value of returns
- Central moments are measured relative to the mean (i.e., central around the mean).
  - first central moment is zero
  - second central moment is the variance of the distribution, which measures the dispersion of data.
  - third central moment measures the departure from symmetry in the distribution. This moment will equal zero for a symmetric distribution (such as the normal distribution).
    - The skewness statistic is the standardized third central moment. Skewness refers to the extent to which the distribution of data is not symmetric around its mean.
  - fourth central moment measures the degree of clustering in the distribution
    - kurtosis statistic is the standardized fourth central moment of the distribution. Kurtosis refers to the degree of peakedness or clustering in the data distribution
    - Kurtosis for the normal distribution equals 3.
      - excess kurtosis = kurtosis — 3

### skewness and kurtosis
#### LO 16.7: Interpret the skewness and kurtosis of a statistical distribution, and interpret the concepts of coskewness and cokurtosis.
- Skewness, or skew, refers to the extent to which a distribution is not symmetrical.
- Skewness affects the location of the mean, median, and mode of a distribution.
  - the mean is pulled in the direction of the skew
- Kurtosis is a measure of the degree to which a distribution is more or less “peaked” than a normal distribution. 
  - Leptokurtic describes a distribution that is more peaked than a normal distribution, 
    - have more returns clustered around the mean and more returns with large deviations from the mean (fatter tails).
  - platykurtic refers to a distribution that is less peaked (or flatter) than a normal distribution.
  - A distribution is mesokurtic if it has the same kurtosis as a normal distribution.
  - A distribution is said to exhibit excess kurtosis if it has either more or less kurtosis than the normal distribution.
- In general, greater positive kurtosis and more negative skew in returns distributions indicates increased risk.

### the best linear unbiased estimator
#### LO 16.8: Describe and interpret the best linear unbiased estimator.
- Point estimates are single (sample) values used to estimate population parameters, and the formula used to compute a point estimate is known as an estimator.
- An **unbiased** estimator is one for which the expected value of the estimator is equal to the parameter you are trying to estimate.
- An unbiased estimator is also **efficient** if the variance of its sampling distribution is smaller than all the other unbiased estimators of the parameter you are trying to estimate.
- A **consistent** estimator is one for which the accuracy of the parameter estimate increases as the sample size increases.
- A point estimate is a **linear** estimator when it can be used as a linear function of sample data.

- best linear unbiased estimator (BLUE)




- Probability distributions
  - Parametric distributions, such as a normal distribution, can be described by using a mathematical function.
  - Nonparametric distributions, such as a historical distribution, cannot be described by using a mathematical function.

#### LO 17.1: Distinguish the key properties among the following distributions: uniform distribution, Bernoulli distribution, Binomial distribution, Poisson distribution, normal distribution, lognormal distribution, Chi-squared distribution, Student s t, and F-distributions, and identify common occurrences of each distribution.


- Bernoulli distributed random variables are commonly used for assessing whether or not a company defaults during a specified time period.
- the Binomial is based on discrete events, while the Poisson is based on continuous events. [Difference between Poisson and Binomial distributions](https://math.stackexchange.com/questions/1050184/difference-between-poisson-and-binomial-distributions)
- A confidence interval is a range of values around the expected outcome within which we expect the actual outcome to be some specified percentage of the time.
- confidence intervals(X mean, s deviation)
  - The 90% confidence interval for r is X - 1.65s to X + 1.65s.
  - The 95% confidence interval for r is X - 1.96s to X + 1.96s.
  - The 99% confidence interval for r is X — 2.58s to X + 2.58s.

- A standard normal distribution (i.e., ^-distribution) is a normal distribution that has been standardized so it has a mean of zero and a standard deviation of 1 \[i.e., N"(0 , 1)].
  - z = (observation — population mean) / standard deviation


### central limit theorem
#### LO 17.2: Describe the central limit theorem and the implications it has when combining independent and identically distributed (i.i.d.) random variables.
#### LO 17.3: Describe i.i.d. random variables and the implications of the i.i.d. assumption when combining random variables. 
- The variance of the distribution of sample means is the population variance divided by the sample size.

- Student’s t-distribution, or simply the t-distribution, is a bell-shaped probability distribution that is symmetrical about its mean.
  - It is the appropriate distribution to use when constructing confidence intervals based on small samples (n < 30) from populations with unknown variance and a normal, or approximately normal, distribution.
  - It is defined by a single parameter, the degrees of freedom (df), where the degrees of freedom are equal to the number of sample observations minus 1, n — 1, for sample means.
  - As the degrees of freedom for the t-distribution increase, however, its shape approaches that of the normal distribution.
  - The degrees of freedom for tests based on sample means are n — 1 because, given the mean, only n — 1 observations can be unique!!!

- The chi-square distribution is asymmetrical, bounded below by zero, and approaches the normal distribution in shape as the degrees of freedom increase.
  - The chi-squared test compares the test statistic to a critical chi-squared value at a given level of significance to determine whether to reject or fail to reject a null hypothesis.

- the hypotheses concerned with the equality of the variances of two populations are tested with an F-distributed test statistic. Hypothesis testing using a test statistic that follows an F-distribution is referred to as the F-test.
- The F-test is used under the assumption that the populations from which samples are drawn are normally distributed and that the samples are independent.

### mixture distribution
#### LO 17.4: Describe a mixture distribution and explain the creation and characteristics of mixture distributions.


- Bayes’ theorem is used to update a given set of prior probabilities for a given event in response to the arrival of new information.

### Bayes’ theorem
#### LO 18.1: Describe Bayes’ theorem and apply this theorem in the calculation of conditional probabilities.
- conditional probability
- The Bayesian approach requires a beginning assumption regarding probabilities.

### Bayesian approach VS firequentist approach
#### LO 18.2: Compare the Bayesian approach to the firequentist approach.
- The frequentist approach involves drawing conclusions from sample data based on the frequency of that data.
- With small sample sizes, such as three years of historical performance, the Bayesian approach is often used in practice. With larger sample sizes, most analysts tend to use the frequentist approach.


### Bayes’ theorem with multiple states
#### LO 18.3: Apply Bayes’ theorem to scenarios with more than two possible outcomes and calculate posterior probabilities.

- posterior probabilities


- sampling error of the mean = sample mean — population mean
#### LO 19.1: Calculate and interpret the sample mean and sample variance.
- Dispersion is defined as the variability around the central tendency.
- the central tendency is the measure of the reward and dispersion is a measure of risk.
- Using n — 1 instead of n in the denominator, however, improves the statistical properties of s^2 as an estimator of c^2.
  - an unbiased estimator of the population variance
- The standard error of the sample mean is the standard deviation of the distribution of the sample means.
  - standard error of the sample mean = standard deviation of the population / (size of the sample)^.5
- The covariance captures the linear relationship between one variable and another.

### confidence interval
#### LO 19.2: Construct and interpret a confidence interval.
- Confidence interval estimates result in a range of values within which the actual value of a parameter will lie, given the probability of 1 — a . 
- Here, alpha, a , is called the level of significance for the confidence interval, and the probability 1 — a is referred to as the degree of confidence.
- point estimate ± (reliability factor x standard error)
  - 1.65, 1.96, 2.58
- If the distribution of the population is normal with unknown variance, we can use the t-distribution to construct a confidence interval
- If the d istr ib u tion is n o n n o rm a l but the p o p u la tio n va r ia n ce is known, the ^-statistic can be used as long as the sample size is large (n > 30).


### hypothesis testing
#### LO 19.3: Construct an appropriate null and alternative hypothesis, and calculate an appropriate test statistic.
- Hypothesis testing is the statistical assessment of a statement or idea regarding a population.
- A hypothesis is a statement about the value of a population parameter developed for the purpose of testing a theory or belief.
- The null hypothesis, designated H0, is the hypothesis the researcher wants to reject.
  - The null hypothesis always includes the “equal to ” condition.
  - support or reject
- The alternative hypothesis, designated HA, is what is concluded if there is sufficient evidence to reject the null hypothesis.
- Hypothesis testing involves two statistics: the test statistic calculated from the sample data and the critical value of the test statistic. The value of the computed test statistic relative to the critical value is a key step in assessing the validity of a hypothesis.


### one-tailed and a two-tailed test of hypothesis 
#### LO 19.4: Differentiate between a one-tailed and a two-tailed test and identify when to use each test.
- Type I error: the rejection of the null hypothesis when it is actually true.
  - The significance level is the probability of making a Type I error
- Type II error: the failure to reject the null hypothesis when it is actually false.
  - The power of a test is actually one minus the probability of making a Type II error, or 1 - P(Type II error)
- A confidence interval is a range of values within which the researcher believes the true population parameter may lie.
- Statistical significance does not necessarily imply economic significance.

- The p-value is the probability of obtaining a test statistic that would lead to a rejection of the null hypothesis, assuming the null hypothesis is true.
  - It is the smallest level of significance for which the null hypothesis can be rejected.

#### LO 19.5: Interpret the results of hypothesis tests with a specific level of confidence.
- The chi-squared test is used for hypothesis tests concerning the variance of a normally distributed population.
- The hypotheses concerned with the equality of the variances of two populations are tested with an T-distributed test statistic.
  - normally distributed and that the samples are independent.
- Chebyshev’s inequality states that for any set of observations, whether sample or population data and regardless of the shape of the distribution, the percentage of the observations that lie within k standard deviations of the mean is at least 1 - 1 /k^2 for all k > 1.

### backtesting
#### LO 19.6: Demonstrate the process of backtesting VaR by calculating the number of exceedances.
- The process of backtesting involves comparing expected outcomes against actual data.
- When the VaR measure is exceeded during a given testing period, it is known as an exception or an exceedance.
- One of the main issues with backtesting VaR models is that exceptions are often serially correlated.
  - In other words, there is a high probability that an exception will occur after the previous period had an exception. 
- Another issue is that the occurrence of exceptions tends to be correlated with overall market volatility. 
  - In other words, VaR exceptions tend to be higher (lower) when market volatility is high (low). 
- This may be the result of a VaR model failing to quickly react to changes in risk levels.


- Linear regression refers to the process of representing relationships with linear equations where there is one dependent variable being explained by one or more independent variables.

### regression analysis
#### LO 20.1: Explain how regression analysis in econometrics measures the relationship between dependent and independent variables.
- A regression analysis has the goal of measuring how changes in one variable, called a dependent or explained variable can be explained by changes in one or more other variables called the independent or explanatory variables.


### population regression function
#### LO 20.2: Interpret a population regression function, regression coefficients, parameters, slope, intercept, and the error term.
- population regression function would consist of parameters called regression coefficients
- error term or noise component denoted £j.
- Often, it is found that limiting an equation to the one or two independent variables with the most explanatory power is the best choice.

### sample regression function
#### LO 20.3: Interpret a sample regression function, regression coefficients, parameters, slope, intercept, and the error term.
- an extra term on the end called the residual: ei = Yi — (b0 + b1 x Xi), ei = £i


### properties of regression
#### LO 20.4: Describe the key properties of a linear regression.
- when we refer to a linear regression model we generally assume that the equation is linear in the parameters; it may or may not be linear in the variables.

### ordinary least squares (OLS) regression
#### LO 20.5: Define an ordinary least squares (OLS) regression and calculate the intercept and slope of the regression.

- Ordinary least squares (OLS) estimation is a process that estimates the population parameters Bj with corresponding values for b that minimize the squared residuals (i.e., error terms).
- slope coefficient (b1) = Cov(X, Y)/Var(X)
- intercept term (b0) = mean of Y - b1\*mean of X


### assumptions underlying linear regression
#### LO 20.6: Describe the method and three key assumptions of OLS for estimation of parameters.
- The expected value of the error term, conditional on the independent variable, is zero.
- All (X, Y) observations are independent and identically distributed (i.i.d.).
- It is unlikely that large outliers will be observed in the data. Large outliers have the potential to create misleading regression results.


- A linear relationship exists between the dependent and independent variable.
- The model is correctly specified in that it includes the appropriate independent variable and does not omit variables.
- The independent variable is uncorrelated with the error terms.
- The variance of £; is constant for all X;
- No serial correlation of the error terms exists
  - The point being that knowing the value of an error for one observation does not reveal information concerning the value of an error for another observation.
- The error term is normally distributed.

### properties of OLS estimators
#### LO 20.7: Summarize the benefits of using OLS estimators.
- OLS estimated coefficients are unbiased, consistent, and (under special conditions) efficient.

#### LO 20.8: Describe the properties of OLS estimators and their sampling distributions, and explain the properties of consistent estimators in general.
- Note that a general guideline for a large sample size in regression analysis is a sample greater than 100.


### OLS regression results
#### LO 20.9: Interpret the explained sum of squares, the total sum of squares, the residual sum of squares, the standard error of the regression, and the regression R^2.
#### LO 20.10: Interpret the results of an OLS regression.
- The sum of squared residuals (SSR), sometimes denoted SSE, for sum of squared errors, is the sum of squares that results from placing a given intercept and slope coefficient into the equation and computing the residuals, squaring the residuals and summing them.
- The coefficient of determination, represented by R^2, is a measure of the “goodness of fit” of the regression.
- Total sum of squares = explained sum of squares + sum of squared residuals
  - total sum of squares (TSS) is also known as sum of squares total (SST), and explained sum of squares (ESS) is also known as regression sum of squares (RSS)
- R^2 = ESS/TSS = 1 - SSR/TSS
- In a simple two-variable regression, the square root of R2 is the correlation coefficient (r) between Xi and Yi.
- The correlation coefficient is a standard measure of the strength of the linear relationship between two variables.
  - First, the correlation coefficient indicates the sign of the relationship, whereas the coefficient of determination does not.
  - Second, the coefficient of determination can apply to an equation with several independent variables, and it implies a causation or explanatory power, while the correlation coefficient only applies to two variables and does not imply causation between the variables.

- The standard error of the regression (SER) measures the degree of variability of the actual Y-values relative to the estimated Y-values from a regression equation. 
  - The SER gauges the “fit” of the regression line. 
  - The smaller the standard error, the better the fit.

- The SER is the standard deviation of the error terms in the regression. As such, SER is also referred to as the standard error of the residual, or the standard error of estimate (SEE).
- SER will be low (relative to total variability) if the relationship is very strong and high if the relationship is weak.

![formula](/img/TssEssR.PNG)





























