### the time value of money
- time value of money(TVM)
- at the end of the investment horizon (FV - compounding.) or at the beginning of the investment horizon (PV - discounting).
- think of I/Y as the interest rate per compounding period and N as the number of compounding periods under analysis
- keys
  - N = Number of compounding periods.
  - I/Y = Interest rate per compounding period.
  - PV = Present value.
  - FV = Future value.
  - PMT = Annuity payments, or constant periodic cash flow.
  - CPT = Compute.

- Equilibrium interest rates are the required rate of return for a particular investment
  - interest rates = discount rates, opportunity cost of current consumption, required rate of return, and the cost of capital
- The real risk-free rate of interest is a theoretical rate on a single period loan that has no expectation of inflation in it.
  - T-bill(U.S. Treasury bills) rates are nominal risk-free rates because they contain an inflation premium.
- Default risk: The risk that a borrower will not make the promised payments in a timely manner.
- Liquidity risk: The risk of receiving less than fair value for an investment if it must be sold for cash quickly.
- Maturity risk: the prices of longer-term bonds are more volatile than those of shorter-term bonds
- An annuity is a stream of **equal** cash flows that occurs at equal intervals over a given period.
  - The ordinary annuity is the most common type of annuity. It is characterized by cash flows that occur at the end of each compounding period.
  - the PMT variable is a single periodic payment
- A perpetuity is a financial instrument that pays a fixed amount of money at set intervals over an **infinite** period of time.
  - British consul bonds and most preferred stocks are examples of perpetuities since they promise fixed interest or dividend payments forever.
  - PVperp = PMT/(I/Y)
- to find the PV or FV of this cash flow stream, all we need to do is sum the PVs or FVs of the individual cash flows
- since an increase in the frequency of compounding increases the effective rate of interest, it also increases the FV of a given cash flow and decreases the PV of a given cash flow.
  - m equal the number of compounding periods per year,
  - I/Y = the annual interest rate / m
  - N = the number of years x m
- PV and FV must in diff sign

### random variables
#### LO 15.1: Describe and distinguish between continuous and discrete random variables.
- A random variable is an uncertain quantity/number.
- An outcome is an observed value of a random variable.
- An event is a single outcome or a set of outcomes.
- Mutually exclusive events are events that cannot happen at the same time.
- Exhaustive events are those that include all possible outcomes.
- A probability distribution describes the probabilities of all the possible outcomes for a random variable.
- A discrete random variable is one for which the number of possible outcomes can be counted, and for each possible outcome, there is a measurable and positive probability.
- A probability function, denoted p(x), specifies the probability that a random variable is equal to a specific value.
- A continuous random variable is one for which the number of possible outcomes is infinite, even if lower and upper bounds exist.

- It is customary to speak in terms of the probability of a range of possible price change


### distribution function
#### LO 15.2: Define and distinguish between the probability density function, the
cumulative distribution function, and the inverse cumulative distribution function.

- A probability density function (pdf) is a function, denoted f(x), that can be used to generate the probability that outcomes of a continuous distribution lie within a particular range of outcomes.
  - For a discrete distribution, it is the equivalent of a probability function for a discrete distribution.
  - for a continuous distribution, the probability
of any one particular outcome (of the infinite possible outcomes) is zero. A pdf is used to calculate the probability of an outcome between two values
- A cumulative distribution function (cdf), or simply distribution function, defines the probability that a random variable, X, takes on a value equal to or less than a specific value, x.
- inverse cumulative distribution function can be used to find the value that corresponds to a specific probability.


### discrete probability function
#### LO 15.3: Calculate the probability of an event given a discrete probability function.
- A discrete uniform random variable is one for which the probabilities for all possible outcomes for a discrete random variable are equal.

### conditional probability
#### LO 15.6: Define and calculate a conditional probability, and distinguish between conditional and unconditional probabilities.
- Unconditional probability (i.e., marginal probability) refers to the probability of an event regardless of the past or future occurrence of other events.
- A conditional probability is one where the occurrence of one event affects the probability of the occurrence of another event.
  - P(A | B), where the vertical bar (|) indicates “given,” or “conditional upon.”
  - A conditional probability of an occurrence is also called its likelihood.
- The joint probability of two events is the probability that they will both occur.
  - multiplication rule o f probability.
  - P(AB) = P(A | B) x P(B)

### independent and mutually exclusive events
#### LO 15.4: Distinguish between independent and mutually exclusive events.
- Independent events refer to events for which the occurrence of one has no influence on the occurrence of the others.
- The addition rule fo r probabilities is used to determine the probability that at least one of two events will occur.

### calculating a joint probability of anu number of independent events
#### LO 15.5: Define joint probability, describe a probability matrix, and calculate joint probabilities using probability matrices.
- Joint probabilities of independent events can be conveniently summarized using a probability matrix

- statistics
  - data
  - Statistical methods
    - Descriptive statistics are used to summarize the important characteristics of large data sets.
    - Inferential statistics, which will be discussed in subsequent topics, pertain to the procedures used to make forecasts, estimates, or judgments about a large set of data on the basis of the statistical characteristics of a smaller set (a sample).
- A population is defined as the set of all possible members of a stated group.
- A sample is defined as a subset of the population of interest.


### measure of central tendency
#### LO 16.1: Interpret and apply the mean, standard deviation, and variance of a random variable.
#### LO 16.2: Calculate the mean, standard deviation, and variance of a discrete random variable.
- The sum of the deviations of each observation in the data set from the mean is always zero.
- The median is the midpoint of a data set when the data is arranged in ascending or descending order.
- The mode is the value that occurs most frequently in a data set.
  - unimodal, bimodal, trimodal for one,two three value
- The geometric mean is often used when calculating investment returns over multiple periods or when measuring compound growth rates.
  - When calculating the geometric mean for a returns data set, it is necessary to add 1 to each value under the radical and then subtract 1 from the result.
  - The geom etric mean is always less than or equal to the arithmetic mean, and the difference increases as the dispersion o f the observations increases.

### expectations
#### LO 16.3: Interpret and calculate the expected value of a discrete random variable.
#### LO 16.5: Calculate the mean and variance of sums of variables.

- E(X^2) =/= [E(X)]^2
- The mean and variance of a distribution are defined as the first and second moments of the distribution, respectively.
- Var(X) = E\[(X - E(X))^2] = E(X^2) - \[E(X)]^2
- Var(aX + c) = a2 x Var(X)
- Var(X - Y) = Var(X) + Var(Y), X, Y independent
- The variance and standard deviation measure the dispersion, or volatility, of only one variable.

### covariance and correlation
#### LO 16.4: Calculate and interpret the covariance and correlation between two random variables.
- Covariance is the expected value of the product of the deviations of the two random variables from their respective expected values.
- Cov(Ri, Rj) = E{ \[Ri - E(Ri)]\[Rj - E(Rj)] } = E(Ri, Rj) - E(Ri)*E(Rj)
- Cov(X,X) = Var(X)
- Cov(a + bX, c + dY) = b x d x Cov(X,Y)
- Correlation measures the strength of the linear relationship between two random variables.
  - ranges from -1 to +1.
  - has no units.


### central moments
#### LO 16.6: Describe the four central moments of a statistical variable or distribution: mean, variance, skewness and kurtosis.
- Raw moments are measured relative to an expected value raised to the appropriate power. 
  - The first raw moment is the mean of the distribution, which is the expected value of returns
- Central moments are measured relative to the mean (i.e., central around the mean).
  - first central moment is zero
  - second central moment is the variance of the distribution, which measures the dispersion of data.
  - third central moment measures the departure from symmetry in the distribution. This moment will equal zero for a symmetric distribution (such as the normal distribution).
    - The skewness statistic is the standardized third central moment. Skewness refers to the extent to which the distribution of data is not symmetric around its mean.
  - fourth central moment measures the degree of clustering in the distribution
    - kurtosis statistic is the standardized fourth central moment of the distribution. Kurtosis refers to the degree of peakedness or clustering in the data distribution
    - Kurtosis for the normal distribution equals 3.
      - excess kurtosis = kurtosis — 3

### skewness and kurtosis
#### LO 16.7: Interpret the skewness and kurtosis of a statistical distribution, and interpret the concepts of coskewness and cokurtosis.
- Skewness, or skew, refers to the extent to which a distribution is not symmetrical.
- Skewness affects the location of the mean, median, and mode of a distribution.
  - the mean is pulled in the direction of the skew
- Kurtosis is a measure of the degree to which a distribution is more or less “peaked” than a normal distribution. 
  - Leptokurtic describes a distribution that is more peaked than a normal distribution, 
    - have more returns clustered around the mean and more returns with large deviations from the mean (fatter tails).
  - platykurtic refers to a distribution that is less peaked (or flatter) than a normal distribution.
  - A distribution is mesokurtic if it has the same kurtosis as a normal distribution.
  - A distribution is said to exhibit excess kurtosis if it has either more or less kurtosis than the normal distribution.
- In general, greater positive kurtosis and more negative skew in returns distributions indicates increased risk.

### the best linear unbiased estimator
#### LO 16.8: Describe and interpret the best linear unbiased estimator.
- Point estimates are single (sample) values used to estimate population parameters, and the formula used to compute a point estimate is known as an estimator.
- An **unbiased** estimator is one for which the expected value of the estimator is equal to the parameter you are trying to estimate.
- An unbiased estimator is also **efficient** if the variance of its sampling distribution is smaller than all the other unbiased estimators of the parameter you are trying to estimate.
- A **consistent** estimator is one for which the accuracy of the parameter estimate increases as the sample size increases.
- A point estimate is a **linear** estimator when it can be used as a linear function of sample data.

- best linear unbiased estimator (BLUE)

![formula](/img/stats.PNG)


- Probability distributions
  - Parametric distributions, such as a normal distribution, can be described by using a mathematical function.
  - Nonparametric distributions, such as a historical distribution, cannot be described by using a mathematical function.

#### LO 17.1: Distinguish the key properties among the following distributions: uniform distribution, Bernoulli distribution, Binomial distribution, Poisson distribution, normal distribution, lognormal distribution, Chi-squared distribution, Student s t, and F-distributions, and identify common occurrences of each distribution.


- Bernoulli distributed random variables are commonly used for assessing whether or not a company defaults during a specified time period.
- the Binomial is based on discrete events, while the Poisson is based on continuous events. [Difference between Poisson and Binomial distributions](https://math.stackexchange.com/questions/1050184/difference-between-poisson-and-binomial-distributions)
- A confidence interval is a range of values around the expected outcome within which we expect the actual outcome to be some specified percentage of the time.
- confidence intervals(X mean, s deviation)
  - The 90% confidence interval for r is X - 1.65s to X + 1.65s.
  - The 95% confidence interval for r is X - 1.96s to X + 1.96s.
  - The 99% confidence interval for r is X — 2.58s to X + 2.58s.

- A standard normal distribution (i.e., ^-distribution) is a normal distribution that has been standardized so it has a mean of zero and a standard deviation of 1 \[i.e., N"(0 , 1)].
  - z = (observation — population mean) / standard deviation


### central limit theorem
#### LO 17.2: Describe the central limit theorem and the implications it has when combining independent and identically distributed (i.i.d.) random variables.
#### LO 17.3: Describe i.i.d. random variables and the implications of the i.i.d. assumption when combining random variables. 
- The variance of the distribution of sample means is the population variance divided by the sample size.

- Student’s t-distribution, or simply the t-distribution, is a bell-shaped probability distribution that is symmetrical about its mean.
  - It is the appropriate distribution to use when constructing confidence intervals based on small samples (n < 30) from populations with unknown variance and a normal, or approximately normal, distribution.
  - It is defined by a single parameter, the degrees of freedom (df), where the degrees of freedom are equal to the number of sample observations minus 1, n — 1, for sample means.
  - As the degrees of freedom for the t-distribution increase, however, its shape approaches that of the normal distribution.
  - The degrees of freedom for tests based on sample means are n — 1 because, given the mean, only n — 1 observations can be unique!!!

- The chi-square distribution is asymmetrical, bounded below by zero, and approaches the normal distribution in shape as the degrees of freedom increase.
  - The chi-squared test compares the test statistic to a critical chi-squared value at a given level of significance to determine whether to reject or fail to reject a null hypothesis.

- the hypotheses concerned with the equality of the variances of two populations are tested with an F-distributed test statistic. Hypothesis testing using a test statistic that follows an F-distribution is referred to as the F-test.
- The F-test is used under the assumption that the populations from which samples are drawn are normally distributed and that the samples are independent.

### mixture distribution
#### LO 17.4: Describe a mixture distribution and explain the creation and characteristics of mixture distributions.


- Bayes’ theorem is used to update a given set of prior probabilities for a given event in response to the arrival of new information.

### Bayes’ theorem
#### LO 18.1: Describe Bayes’ theorem and apply this theorem in the calculation of conditional probabilities.
- conditional probability
- The Bayesian approach requires a beginning assumption regarding probabilities.

### Bayesian approach VS firequentist approach
#### LO 18.2: Compare the Bayesian approach to the firequentist approach.
- The frequentist approach involves drawing conclusions from sample data based on the frequency of that data.
- With small sample sizes, such as three years of historical performance, the Bayesian approach is often used in practice. With larger sample sizes, most analysts tend to use the frequentist approach.


### Bayes’ theorem with multiple states
#### LO 18.3: Apply Bayes’ theorem to scenarios with more than two possible outcomes and calculate posterior probabilities.

- posterior probabilities


- sampling error of the mean = sample mean — population mean
#### LO 19.1: Calculate and interpret the sample mean and sample variance.
- Dispersion is defined as the variability around the central tendency.
- the central tendency is the measure of the reward and dispersion is a measure of risk.
- Using n — 1 instead of n in the denominator, however, improves the statistical properties of s^2 as an estimator of c^2.
  - an unbiased estimator of the population variance
- The standard error of the sample mean is the standard deviation of the distribution of the sample means.
  - standard error of the sample mean = standard deviation of the population / (size of the sample)^.5
- The covariance captures the linear relationship between one variable and another.

### confidence interval
#### LO 19.2: Construct and interpret a confidence interval.
- Confidence interval estimates result in a range of values within which the actual value of a parameter will lie, given the probability of 1 — a . 
- Here, alpha, a , is called the level of significance for the confidence interval, and the probability 1 — a is referred to as the degree of confidence.
- point estimate ± (reliability factor x standard error)
  - 1.65, 1.96, 2.58
- If the distribution of the population is normal with unknown variance, we can use the t-distribution to construct a confidence interval
- If the d istr ib u tion is n o n n o rm a l but the p o p u la tio n va r ia n ce is known, the ^-statistic can be used as long as the sample size is large (n > 30).


### hypothesis testing
#### LO 19.3: Construct an appropriate null and alternative hypothesis, and calculate an appropriate test statistic.
- Hypothesis testing is the statistical assessment of a statement or idea regarding a population.
- A hypothesis is a statement about the value of a population parameter developed for the purpose of testing a theory or belief.
- The null hypothesis, designated H0, is the hypothesis the researcher wants to reject.
  - The null hypothesis always includes the “equal to ” condition.
  - support or reject
- The alternative hypothesis, designated HA, is what is concluded if there is sufficient evidence to reject the null hypothesis.
- Hypothesis testing involves two statistics: the test statistic calculated from the sample data and the critical value of the test statistic. The value of the computed test statistic relative to the critical value is a key step in assessing the validity of a hypothesis.


### one-tailed and a two-tailed test of hypothesis 
#### LO 19.4: Differentiate between a one-tailed and a two-tailed test and identify when to use each test.
- Type I error: the rejection of the null hypothesis when it is actually true.
  - The significance level is the probability of making a Type I error
- Type II error: the failure to reject the null hypothesis when it is actually false.
  - The power of a test is actually one minus the probability of making a Type II error, or 1 - P(Type II error)
- A confidence interval is a range of values within which the researcher believes the true population parameter may lie.
- Statistical significance does not necessarily imply economic significance.

- The p-value is the probability of obtaining a test statistic that would lead to a rejection of the null hypothesis, assuming the null hypothesis is true.
  - It is the smallest level of significance for which the null hypothesis can be rejected.

#### LO 19.5: Interpret the results of hypothesis tests with a specific level of confidence.
- The chi-squared test is used for hypothesis tests concerning the variance of a normally distributed population.
- The hypotheses concerned with the equality of the variances of two populations are tested with an T-distributed test statistic.
  - normally distributed and that the samples are independent.
- Chebyshev’s inequality states that for any set of observations, whether sample or population data and regardless of the shape of the distribution, the percentage of the observations that lie within k standard deviations of the mean is at least 1 - 1 /k^2 for all k > 1.

### backtesting
#### LO 19.6: Demonstrate the process of backtesting VaR by calculating the number of exceedances.
- The process of backtesting involves comparing expected outcomes against actual data.
- When the VaR measure is exceeded during a given testing period, it is known as an exception or an exceedance.
- One of the main issues with backtesting VaR models is that exceptions are often serially correlated.
  - In other words, there is a high probability that an exception will occur after the previous period had an exception. 
- Another issue is that the occurrence of exceptions tends to be correlated with overall market volatility. 
  - In other words, VaR exceptions tend to be higher (lower) when market volatility is high (low). 
- This may be the result of a VaR model failing to quickly react to changes in risk levels.


- Linear regression refers to the process of representing relationships with linear equations where there is one dependent variable being explained by one or more independent variables.

![formula](/img/test.PNG)

### regression analysis
#### LO 20.1: Explain how regression analysis in econometrics measures the relationship between dependent and independent variables.
- A regression analysis has the goal of measuring how changes in one variable, called a dependent or explained variable can be explained by changes in one or more other variables called the independent or explanatory variables.


### population regression function
#### LO 20.2: Interpret a population regression function, regression coefficients, parameters, slope, intercept, and the error term.
- population regression function would consist of parameters called regression coefficients
- error term or noise component denoted £j.
- Often, it is found that limiting an equation to the one or two independent variables with the most explanatory power is the best choice.

### sample regression function
#### LO 20.3: Interpret a sample regression function, regression coefficients, parameters, slope, intercept, and the error term.
- an extra term on the end called the residual: ei = Yi — (b0 + b1 x Xi), ei = £i


### properties of regression
#### LO 20.4: Describe the key properties of a linear regression.
- when we refer to a linear regression model we generally assume that the equation is linear in the parameters; it may or may not be linear in the variables.

### ordinary least squares (OLS) regression
#### LO 20.5: Define an ordinary least squares (OLS) regression and calculate the intercept and slope of the regression.

- Ordinary least squares (OLS) estimation is a process that estimates the population parameters Bj with corresponding values for b that minimize the squared residuals (i.e., error terms).
- slope coefficient (b1) = Cov(X, Y)/Var(X)
- intercept term (b0) = mean of Y - b1\*mean of X


### assumptions underlying linear regression
#### LO 20.6: Describe the method and three key assumptions of OLS for estimation of parameters.
- The expected value of the error term, conditional on the independent variable, is zero.
- All (X, Y) observations are independent and identically distributed (i.i.d.).
- It is unlikely that large outliers will be observed in the data. Large outliers have the potential to create misleading regression results.


- A linear relationship exists between the dependent and independent variable.
- The model is correctly specified in that it includes the appropriate independent variable and does not omit variables.
- The independent variable is uncorrelated with the error terms.
- The variance of £; is constant for all X;
- No serial correlation of the error terms exists
  - The point being that knowing the value of an error for one observation does not reveal information concerning the value of an error for another observation.
- The error term is normally distributed.

### properties of OLS estimators
#### LO 20.7: Summarize the benefits of using OLS estimators.
- OLS estimated coefficients are unbiased, consistent, and (under special conditions) efficient.

#### LO 20.8: Describe the properties of OLS estimators and their sampling distributions, and explain the properties of consistent estimators in general.
- Note that a general guideline for a large sample size in regression analysis is a sample greater than 100.


### OLS regression results
#### LO 20.9: Interpret the explained sum of squares, the total sum of squares, the residual sum of squares, the standard error of the regression, and the regression R^2.
#### LO 20.10: Interpret the results of an OLS regression.
- The sum of squared residuals (SSR), sometimes denoted SSE, for sum of squared errors, is the sum of squares that results from placing a given intercept and slope coefficient into the equation and computing the residuals, squaring the residuals and summing them.
- The coefficient of determination, represented by R^2, is a measure of the “goodness of fit” of the regression.
- Total sum of squares = explained sum of squares + sum of squared residuals
  - total sum of squares (TSS) is also known as sum of squares total (SST), and explained sum of squares (ESS) is also known as regression sum of squares (RSS)
- R^2 = ESS/TSS = 1 - SSR/TSS
- In a simple two-variable regression, the square root of R2 is the correlation coefficient (r) between Xi and Yi.
- The correlation coefficient is a standard measure of the strength of the linear relationship between two variables.
  - First, the correlation coefficient indicates the sign of the relationship, whereas the coefficient of determination does not.
  - Second, the coefficient of determination can apply to an equation with several independent variables, and it implies a causation or explanatory power, while the correlation coefficient only applies to two variables and does not imply causation between the variables.

- The standard error of the regression (SER) measures the degree of variability of the actual Y-values relative to the estimated Y-values from a regression equation. 
  - The SER gauges the “fit” of the regression line. 
  - The smaller the standard error, the better the fit.

- The SER is the standard deviation of the error terms in the regression. As such, SER is also referred to as the standard error of the residual, or the standard error of estimate (SEE).
- SER will be low (relative to total variability) if the relationship is very strong and high if the relationship is weak.

![formula](/img/TssEssR.PNG)


### regression coefficients confidence intervals 
#### LO 21.1: Calculate, and interpret confidence intervals for regression coefficients.
- as SER rises, Sb1also increases, and the confidence interval widens

### regression coefficients hypothesis testing
#### LO 21.3: Interpret hypothesis tests about regression coefficients.
- t-test for B1
  - Letting bj be the point estimate for Bp the appropriate test statistic with n — 2 degrees of freedom: 
    - t = (b1 - B1)/Sb1
#### LO 21.2: Interpret the p-value.
- the p-value is the smallest level of significance for which the null hypothesis can be rejected.
  - gives researchers a general idea of statistical significance without selecting a significance level.
- Independent variables that fall into this category are called dummy variables and are often used to quantify the impact of qualitative events.
  - binary
  - Use one less dummy variable than the number of categories

### heteroskedasticity
#### LO 21.4: Evaluate the implications of homoskedasticity and heteroskedasticity.
- If the variance of the residuals is constant across all observations in the sample, the regression is said to be homoskedastic. 
- When the opposite is true, the regression exhibits heteroskedasticity, which occurs when the variance of the residuals is not the same across all observations in the sample.
- Unconditional heteroskedasticity occurs when the heteroskedasticity is not related to the level of the independent variables,
- Conditional heteroskedasticity is heteroskedasticity that is related to the level of (i.e., conditional on) the independent variable.
  - Conditional heteroskedasticity does create significant problems fo r statistical inference.

- The most common remedy, however, is to calculate robust standard errors. These robust standard errors are used to recalculate the f-statistics using the original regression coefficients.

### the Gauss-Markov Theorem
#### LO 21.5: Determine the conditions under which the OLS is the best linear conditionally unbiased estimator.
#### LO 21.6: Explain the Gauss-Markov Theorem and its limitations, and alternatives to the OLS.

- The Gauss-Markov theorem says that if the linear regression model assumptions are true and the regression errors display homoskedasticity, then the OLS estimators have the following properties.
  - 1. The OLS estimated coefficients have the minimum variance compared to other methods of estimating the coefficients (i.e., they are the most precise).
  - 2. The OLS estimated coefficients are based on linear functions.
  - 3. The OLS estimated coefficients are unbiased, which means that in repeated sampling the averages of the coefficients from the sample will be distributed around the true population parameters 
  - 4. The OLS estimate of the variance of the errors is unbiased.

### small sample size
#### LO 21.7: Apply and interpret the t-statistic when the sample size is small.


### omitted variable bias
#### LO 22.1: Define and interpret omitted variable bias, and describe the methods for addressing this bias.
- Omitted variable bias is present when two conditions are met: 
  - (1) the omitted variable is correlated with the movement of the independent variable in the model
  - (2) the omitted variable is a determinant of the dependent variable.

- Multiple regression analysis is therefore used to eliminate omitted variable bias since it can estimate the effect of one independent variable on the dependent variable while holding all other variables constant.


#### LO 22.2: Distinguish between single and multiple regression.
- Multiple regression is regression analysis with more than one independent variable.


#### LO 22.5: Describe the OLS estimator in a multiple regression.

#### LO 22.3: Interpret the slope coefficient in a multiple regression.
- partial slope coefficients

#### LO 22.4: Describe homoskedasticity and heteroskedasticity in a multiple regression.
- Homoskedasticity refers to the condition that the variance of the error term is constant for all independent variables, X, from i = 1 to n
- Heteroskedasticity means that the dispersion of the error terms varies over the sample. It may take the form of conditional heteroskedasticity, which says that the variance is a function of the independent variables.

#### LO 22.6: Calculate and interpret measures of fit in multiple regression.
- The standard error of the regression (SER) measures the uncertainty about the accuracy A of the predicted values of the dependent variable
  - Se
- regression minimizes SSR(sum of squared residuals)
- SER = \[SSR/(n-k-1)]^0.5
  - n= number of observations
  - k= number of independent variables
- SER measures the degree of variability of the actual Y-values relative to the estimated Y values. 
  - The SER gauges the “fit” of the regression line. 
  - The smaller the standard error, the better the jit.

- The multiple coefficient of determination, R^2, can be used to test the overall effectiveness of the entire set of independent variables in explaining the dependent variable.
- overestimating the regression as R2 almost always increases as independent variables are added to the model
- R^2 = (TSS-SSR)/TSS
- Adjusted Ra^2 = 1- \[(n-1)/(n-k-1) * (1-R^2)]
  - Adjusted R2 must be less than or equal to R2

#### LO 22.8: Explain the concept of imperfect and perfect multicollinearity and their implications.
- The degree of correlation will determine the difference between perfect and imperfect multicollinearity.
  - If one of the independent variables is a perfect linear combination of the other independent variables, then the model is said to exhibit perfect multicollinearity. -> it will not be possible to find the OLS estimators necessary for the regression results.
- dummy variable trap: if every observation is linked to only one class, all dummy variables are included as regressors, and an intercept term exists, then the regression will exhibit perfect multicollinearity.
  - Whenever we want to distinguish between n classes, we must use n — 1 dummy variables. ->the intercept term will represent the omitted class.
- Imperfect multicollinearity arises when two or more independent variables are highly correlated, but less than perfectly correlated.

- As a result of multicollinearity, there is a greater probability that we will incorrectly conclude that a variable is not statistically significant(e.g., a Type II error).

- The most common way to detect multicollinearity is the situation where t-tests indicate that none of the individual coefficients is significantly different than zero, while the R2 is high.
- If the absolute value of the sample correlation between any two independent variables in the regression is greater than 0.7, multicollinearity is a potential problem(two independent variables case).
-High correlation among the independent variables suggests the possibility of multicollinearity, but low correlation among the independent variables does not necessarily indicate multicollinearity is not present.

- The most common method to correct for multicollinearity is to omit one or more of the correlated independent variables.
  - stepwise regression


#### LO 23.1: Construct, apply, and interpret hypothesis tests and confidence intervals for a single coefficient in a multiple regression.
- Hypothesis Testing of Regression Coefficients
  - t = (estimated regression coefficient — hypothesized value) /coefficient standard error of bj
  - n-k-1 degrees of freedom

- Determining Statistical Significance
  - H0:bj = 0 versus Ha; bj=/=0

- Interpreting p-Values
  - If the p-value is less than significance level, the null hypothesis can be rejected.

- Confidence Intervals for a Regression Coefficient
  - estimated regression coefficient ± (critical t-value) (coefficient standard error)

#### LO 23.2: Construct, apply, and interpret joint hypothesis tests and confidence intervals for multiple coefficients in a multiple regression.
#### LO 23.3: Interpret the F-statistic.
#### LO 23.5: Interpret confidence sets for multiple coefficients.
- A robust method for applying joint hypothesis testing, especially when independent variables are correlated, is known as the T-statistic.
- An F-test assesses how well the set of independent variables, as a group, explains the variation in the dependent variable.
  - always one tailed test
  - (ESS/k)/(SSR/n-k-1)
  - an analysis of variance (ANOVA) table
- Specification bias refers to how the slope coefficient and other statistics for a given independent variable are usually different in a simple regression when compared to those of the same variable when included in a multiple regression.
- Specification bias is indicated by the extent to which the coefficient for each independent variable is different when compared across equations


#### LO 23.7: Interpret the R2 and adjusted R2 in a multiple regression.
- A restricted least squares regression imposes a value on one or more coefficients with the goal of analyzing if the restriction is significant.


#### LO 23.4: Interpret tests of a single restriction involving multiple coefficients.

#### LO 23.6: Identify examples of omitted variable bias in multiple regressions.
- omitted variable bias in multiple regressions will result if the following two conditions occur:
  - The omitted variable is a determinant of the dependent variable.
  - The omitted variable is correlated with at least one of the independent variables.

#### LO 24.1: Describe linear and nonlinear trends.
- A tim e series is a set of observations for a variable over successive periods of time. The series has a trend if a consistent pattern can be seen by plotting the data
- a slow evolution of variables
-The use of the transformed data produces a linear trend line with a better fit for the data, which increases the predictive ability of the model.

#### LO 24.2: Describe trend models to estimate and forecast trends.
- The bottom line is that when a variable grows at a constant rate, a log-linear model is most appropriate. When the variable increases over time by a constant amount, a linear trend model is most appropriate.


#### LO 24.3: Compare and evaluate model selection criteria, including mean squared error (MSE), s , the Akaike information criterion (AIC), and the Schwarz information criterion (SIC).

- Mean squared error (MSE) is a statistical measure computed as the sum of squared residuals divided by the total number of observations in the sample.
- the regression model with the smallest MSE is also the one that has the largest R2.

- A better methodology to select the best forecasting model is to find the model with the smallest out-of-sample, one-step-ahead MSE.
- The s2 measure is an unbiased estimate of the MSE because it corrects for degrees of freedom(T-k)
  - the s2 and adjusted R2 criteria will always rank forecasting models equivalently.
- As more variables are included in a regression equation, the model is at greater risk of over-fitting the in-sample data. This problem is also often referred to as data mining.

- penalty factors
  - s^2 : T/(T-k)
  - Akaike information criterion (AIC): e^(2k/T)
  - Schwarz information criterion (SIC): T^(k/T)

#### LO 24.4: Explain the necessary conditions for a model selection criterion to demonstrate consistency.
- Consistency is a key property that is used to compare different selection criteria.
- Asymptotic efficiency is the property that chooses a regression model with one-step-ahead forecast error variances closest to the variance of the true model.
- Adjusting for the degrees of freedom is extremely important and the SIC is the best selection criteria because it is consistent and also has the highest penalty factor. The AIC is also an important measure that is often considered in addition to SIC.

- A selection criteria is considered to be consistent if the following two conditions are met:
  - When the true model or data generating process (DGP) is one of the defined regression models under consideration, then the probability of selecting the true model approaches one as the sample size increases.
  - When the true model is not one of the defined regression models being considered, then the probability of selecting the best approximation model approaches one as the sample size increases.

#### LO 25.1: Describe the sources of seasonality and how to deal with it in time series analysis.
- Seasonality in a time series is a pattern that tends to repeat from year to year.

- There are two approaches for modeling and forecasting a time series impacted by seasonality: (1) using a seasonally adjusted time series and (2 ) regression analysis with seasonal dummy variables.


- A seasonally adjusted time series is created by removing the seasonal variation from the data. This type of adjustment is commonly made in macroeconomic forecasting where the goal is to only measure the nonseasonal fluctuations of a variable.

#### LO 25.2: Explain how to use regression analysis to model seasonality.
- The concept of seasonal variation can also be extended to account for other types of calendar effects, such as holiday variations (HDV) and trading-day variations (TDV).

- An important consideration when performing multiple regression and modeling seasonality with dummy variables is the number of dummy variables to include in the model. As mentioned, if we include an intercept in our model and there are s seasons, we use s — 1 dummy variables to avoid the problem of (perfect) multicollinearity.

#### LO 25.3: Explain how to construct an h-step-ahead point forecast.

- Along with seasonal and trend components, cycles constitute an essential third component in a forecasting model.
- Cyclicality captures the dynamics of a data series outside of trend or seasonal data.

#### LO 26.1: Define covariance stationary, autocovariance function, autocorrelation function, partial autocorrelation function, and autoregression. 
- Autoregression refers to the process of regressing a variable on lagged or past values of itself. As you will see in the next topic, when the dependent variable for a time series is regressed against one or more lagged values of itself, the resultant model is called as an autoregressive (AR) model.
- A time series is covariance stationary if its mean, variance, and covariances with lagged and leading values do not change over time. Covariance stationarity is a requirement for using AR models.
- Autocovariance function refers to the tool used to quantify stability of the covariance structure. Its importance lies in its ability to summarize cyclical dynamics in a series that is covariance stationary.
- Autocorrelation function refers to the degree of correlation and interdependency between data points in a time series. 
- Partial autocorrelation function refers to the partial correlation and interdependency between data in a time series that measures the association between data in a series after controlling for the effects of lagged observations.

#### LO 26.2: Describe the requirements for a series to be covariance stationary.

- Constant and finite expected value. The expected value of the time series is constant over time.
- Constant and finite variance. The time series volatility around its mean (i.e., the distribution of the individual observations around the mean) does not change over time.
- Constant and finite covariance between values at any given lag. The covariance of the time series with leading or lagged values of itself is constant.


#### LO 26.3: Explain the implications of working with models that are not covariance stationary.


#### LO 26.4: Define white noise, and describe independent white noise and normal (Gaussian) white noise.
#### LO 26.5: Explain the characteristics of the dynamic structure of white noise.

- A time series process with a zero mean, constant variance, and no serial correlation is referred to as a white noise process (or zero-mean white noise) .

- Variants of a white noise process include independent white noise and normal white noise. 
  - A time series process that exhibits both serial independence and a lack of serial correlation is referred to as independent white noise (or strong white noise). 
  - A time series process that exhibits serial independence, is serially uncorrelated, and is normally distributed is referred to as normal white noise (or Gaussian white noise) .


#### LO 26.6: Explain how a lag operator works.

- A lag operator quantifies how a time series evolves by lagging a data series. It enables a model to express how past data links to the present and how present data links to the future.
-first-difference operator (A)
- distributed lag: a weighted sum of present and past values in a data series, achieved by lagging present values upon past values.


#### LO 26.7: Describe Wold’s theorem.
#### LO 26.8: Define a general linear process.
#### LO 26.9: Relate rational distributed lags to Wold’s theorem.
- Wold’s representation theorem is a model for the covariance stationary residual (i.e., a model that is constructed after making provisions for trends and seasonal components).
- Wold’s representation utilizes an infinite number of distributed lags, where the one-step-ahead forecasted error terms are known as innovations.

- The general linear process is a component in the creation of forecasting models in a covariance stationary time series.
- Infinite polynomials that are a ratio of finite-order polynomials are known as rational polynomials. The distributed lags constructed from these rational polynomials are known as rational distributed lags.

#### LO 26.10: Calculate the sample mean and sample autocorrelation, and describe the Box-Pierce Q-statistic and the Ljung-Box Q-statistic.
#### LO 26.11: Describe sample partial autocorrelation.
- A Q-statistic can be used to measure the degree to which autocorrelations vary from zero and whether white noise is present in a dataset.
- The Box-Pierce Q-statistic reflects the absolute magnitudes of the correlations, because it sums the squared autocorrelations.

- The Ljung- Box Q-statistic is similar to the Box-Pierce Q-statistic except that it replaces the sum of squared autocorrelations with a weighted sum of squared autocorrelations.

- Both Q-statistics typically arrive at the same result. The Ljung-Box statistic works better with smaller samples of data and replaces the sum of squared autocorrelations in the Box-Pierce statistic with a weighted sum of squared autocorrelations.


#### LO 27.1: Describe the properties of the first-order moving average (MA(1)) process, and distinguish between autoregressive representation and moving average representation.

- The MA(1) process is considered to be first-order because it only has one lagged error term
- One key feature of moving average processes is called the autocorrelation (p) cutoff.

- The problem with a moving average representation of an MA(1) process is that it attempts to estimate a variable in terms of unobservable white noise random shocks.

#### LO 27.2: Describe the properties of a general finite-order process of order q (MA(q)) process.
- MA(q) process experiences autocorrelation cutoff after the qth lagged error term.
- It is the moving average representation that is best at capturing only random movements.

#### LO 27.3: Describe the properties of the first-order autoregressive (AR(1)) process, and define and explain the Yule-Walker equation.
- The firstorder autoregressive \[AR(l)] process must also have a mean of zero and a constant variance.
- In order to estimate the autoregressive parameters, such as the coefficient (cj)), forecasters need to accurately estimate the autocovariance of the data series. The Yule-Walker equation is used for this purpose.
- While autocorrelation cutoff is hallmark o f moving average processes, a gradual decay in autocorrelations is a sure sign that a forecaster is dealing with an autoregressive process.

#### LO 27.4: Describe the properties of a general pth order autoregressive (AR(p)) process.
-The AR(p) process is also covariance stationary if |(/))| < 1 and it exhibits the same decay in autocorrelations that was found in the AR(1) process.

#### LO 27.5: Define and describe the properties of the autoregressive moving average (ARMA) process.

#### LO 27.6: Describe the application of AR and ARMA processes.
- A forecaster might begin by plotting the autocorrelations for a data series and find that the autocorrelations decay gradually or cut off abruptly
- Another way of looking at model applications is to test various models using regression results.

- Seasonality is most apparent when the autocorrelations for a data series do not abruptly cut off, but rather decay gradually with periodic spikes.

#### LO 28.1: Define and distinguish between volatility, variance rate, and implied volatility.

- The volatility of a variable, cr, is represented as the standard deviation of that variable’s continuously compounded return.
- By assuming daily returns are independent with the same level of variation, daily volatility can be extended over a number of days, T, by multiplying the standard deviation of the return by the square root of T. This is known as **the square root o f time rule**.
  - 252 days: the number of business days in a year
- a variable’s variance rate, which is simply the square of volatility

- The implied volatility of an option is computed from an option pricing model, such as the Black-Scholes-Merton (BSM) model.
- The most widely used index for publishing implied volatility is the Chicago Board Options Exchange (CBOE) Volatility Index (ticker symbol: VIX). The VIX demonstrates implied volatility on a wide variety of 30-day calls and puts on the S&P 500 Index.


#### LO 28.2: Describe the power law.
- It is typically assumed that the change in asset prices is normally distributed. This makes it convenient to apply standard deviation when determining confidence intervals for an asset’s price.
- The power law states that when X is large, the value of a variable Khas the following property: P(V > X) = K * X^(-a)
- The power law suggests that extreme movements have a very low probability of occurring


#### LO 28.3: Explain how various weighting schemes can be used in estimating volatility.

#### LO 28.4: Apply the exponentially weighted moving average (EWMA) model to estimate volatility.
#### LO 28.8: Explain the weights in the EWMA and GARCH(1,1) models.
- The exponentially weighted moving average (EWMA)
  - weights are assumed to decline exponentially back through time.

#### LO 28.5: Describe the generalized autoregressive conditional heteroskedasticity (GARCH (p,q)) model for estimating volatility and its properties.
#### LO 28.6: Calculate volatility using the GARCH(1,1) model.

- An additional characteristic of a GARCH(1,1) estimate is the implicit assumption that variance tends to revert to a long-term average level.
- long-run average variance = w/(1-a-b)


#### LO 28.7: Explain mean reversion and how it is captured in the GARCH(1,1) model.
- The sum of a + (3 is called the persistence, and if the model is to be stationary over time (with reversion to the mean), the sum must be less than one. The persistence describes the rate at which the volatility will revert to its long-term value following a large movement.
- one way to estimate volatility (e.g., variance) is to use a maximum likelihood estimator. Maximum likelihood estimators select values of model parameters that maximize the likelihood that the observed data will occur in a sample.

#### LO 28.9: Explain how GARCH models perform in volatility forecasting.
#### LO 28.10: Describe the volatility term structure and the impact of volatility changes.

- The question then arises, if GARCH models do a good job at explaining past volatility, GARCH models do a fine job at forecasting volatility from a volatility term structure perspective

#### LO 29.1: Define correlation and covariance and differentiate between correlation and dependence.
-A correlation of zero between two variables simply implies that there is no linear relationship between the two variables

#### LO 29.2: Calculate covariance using the EWMA and GARCH(1,1) models. 
- Covariance is a statistical measure that is calculated over historical time periods. Conventional wisdom suggests that more recent observations should carry more weight
- updated estimate of covariance, GARCH(1,1)


#### LO 29.3: Apply the consistency condition to covariance.
- A variance-covariance matrix can be constructed using the calculated estimates of variance and covariance rates for a set of variables.
- A matrix is known as positive-semidefinite if it is internally consistent.
- 2 ways

#### LO 29.4: Describe the procedure of generating samples from a bivariate normal distribution.
- 1. Independent samples and Zy are obtained from a univariate standardized normal distribution.
- 2. £x = Zx
- 3. £y = pxyZx + Zy(1-pxy^2)^.5
#### LO 29.5: Describe properties of correlations between normally distributed variables when using a one-factor model.
- A factor model can be used to define correlations between normally distributed variables.
- The most well-known one factor model in finance is the capital asset p ricin g model (CAPM).


#### LO 29.6: Define copula and describe the key properties of copulas and copula correlation.
- A copula creates a joint probability distribution between two or more variables while maintaining their individual marginal distributions.
- The key property of a copula correlation model is the preservation o f the original marginal distributions while defining a correlation between them.
- using a copula is a way to indirectly define a correlation structure between two variables when it is not possible to directly define correlation.

#### LO 29.8: Describe the Gaussian copula, Student’s t-copula, multivariate copula, and one factor copula.
- A Gaussian copula maps the marginal distribution of each variable to the standard normal distribution. The mapping of each variable to the new distribution is done based on percentiles.

- Student’s r-copula
- A multivariate copula is used to define a correlation structure for more than two variables.

#### LO 29.7: Explain tail dependence.

- Student’s r-copula is better than a Gaussian copula in describing the correlation structure of assets that historically have extreme outliers in the distribution tails at the same time

![formula](/img/model1.PNG)
![formula](/img/model2.PNG)
![formula](/img/model3.PNG)

#### LO 30.1: Describe the basic steps to conduct a Monte Carlo simulation.
- four basic steps
  - Step 1: Specify the data generating process (DGP)
  - Step 2: Estimate an unknown variable or parameter
  - Step 3: Save the estimate from step 2
  - Step 4: Go back to step 1 and repeat this process //times

#### LO 30.2: Describe ways to reduce Monte Carlo sampling error.
- The standard error of the true expected value is computed as s / N^.5 , where s is the standard deviation of the output variables and N is the number of scenarios or replications in the simulation.
- increasing the number of generated scenarios can become costly for more complex multi-period simulations.
- The two most commonly used techniques for reducing the standard error estimate are antithetic variates and control variates.


#### LO 30.3: Explain how to use antithetic variate technique to reduce Monte Carlo sampling error.
- antithetic variate technique can reduce Monte Carlo sampling error by rerunning the simulation using a complement set of the original set of random variables.

- By definition, the use of antithetic variates results in a lower covariance and variance, because the two sets are perfectly negatively correlated.

- causes the error terms to be independent for the two sets, which results in a negative covariance term in the variance equation. This negative relationship means that the Monte Carlo sampling error must always be smaller using this approach.

#### LO 30.4: Explain how to use control variates to reduce Monte Carlo sampling error and when it is effective.
- A control variate involves replacing a variable x (under simulation) that has unknown properties with a similar variable y that has known properties.

#### LO 30.5: Describe the benefits of reusing sets of random number draws across Monte Carlo experiments and how to reuse them.
- Two examples of reusing sets of random numbers are for testing the power of the Dickey-Fuller test (used to determine whether a time series is covariance stationary) or for different experiments with options using time series data.

#### LO 30.6: Describe the bootstrapping method and its advantage over Monte Carlo simulation.

- The bootstrapping approach draws random return data from a sample of historical data.

#### LO 30.8: Describe situations where the bootstrapping method is ineffective.
- Two situations that cause the bootstrapping method to be ineffective are outliers in the data and non-independent data.

#### LO 30.7: Describe the pseudo-random number generation method and how a good simulation design alleviates the effects the choice of the seed has on the properties of the generated series.

- A good random number generator has the ability to reproduce a random sequence and analyze characteristics of random numbers.
- A very common pseudo-random number generator is one that generates random number sequences uniformly distributed between 0 and 1.


#### LO 30.9: Describe disadvantages of the simulation approach to financial problem solving.























































